import re
import os
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup

# ================================================
# Step 1: Data Loading and Cleaning
# ================================================

DATA_PATH = 'customer_calls.csv'
df = pd.read_csv(DATA_PATH)

# Display original label distribution
print("Original Label Distribution:")
print(df['label'].value_counts())

def clean_text(text):
    """
    Clean the input text by:
    - Lowercasing
    - Removing URLs and email addresses
    - Removing unwanted characters (retain .,!?)
    - Removing extra spaces
    """
    text = str(text).lower()
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\S+@\S+', '', text)   # Remove email addresses
    text = re.sub(r'[^a-zà-ÿ0-9\s.,!?]', '', text)  # Remove unwanted characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Clean transcriptions
df['transcription'] = df['transcription'].apply(clean_text)
print("\nSample Cleaned Transcriptions:")
print(df['transcription'].head())

# ================================================
# Step 2: Prepare Data for Binary Classification
# ================================================
# We will ignore "no_answer" cases in training and map labels:
# 1: solved, 0: unsolved

train_df = df[df['label'] != 'no_answer'].copy()
label_mapping = {'solved': 1, 'unsolved': 0}
train_df['label_binary'] = train_df['label'].map(label_mapping)
print("\nBinary Label Distribution:")
print(train_df['label_binary'].value_counts())

# (Optional) If you need to augment the data (e.g. for imbalanced classes), you could use nlpaug.
# For example, augmenting the unsolved samples:
import nlpaug.augmenter.word as naw
syn_aug = naw.SynonymAug(aug_p=0.3)  # Augment 30% of the words
unsolved_df = train_df[train_df['label_binary'] == 0]
if not unsolved_df.empty:
    augmented_texts = unsolved_df['transcription'].apply(lambda x: syn_aug.augment(x))
    augmented_unsolved_df = pd.DataFrame({
        'transcription': augmented_texts,
        'label_binary': unsolved_df['label_binary']
    })
    train_df = pd.concat([train_df, augmented_unsolved_df], ignore_index=True)
    print("\nLabel Distribution after Augmentation:")
    print(train_df['label_binary'].value_counts())
else:
    print("\nNo 'unsolved' samples to augment.")

# ================================================
# Step 3: Tokenization using BERTimbau Tokenizer
# ================================================

MODEL_NAME = "neuralmind/bert-base-portuguese-cased"
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)

# Set maximum sequence length to 200 tokens (words may be slightly different from tokens)
MAX_LEN = 200

def tokenize_texts(texts, max_length=MAX_LEN):
    return tokenizer(
        texts.tolist(),
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='pt'
    )

# Split into training and validation sets (stratified)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_df['transcription'].tolist(),
    train_df['label_binary'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=train_df['label_binary']
)

# Tokenize the splits
train_encodings = tokenize_texts(pd.Series(train_texts))
val_encodings = tokenize_texts(pd.Series(val_texts))

# ================================================
# Step 4: Create a Custom Dataset
# ================================================

class CallDataset(Dataset):
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels  # labels can be None for test data

    def __getitem__(self, idx):
        item = {key: self.encodings[key][idx] for key in self.encodings}
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.encodings['input_ids'])

train_dataset = CallDataset(train_encodings, train_labels)
val_dataset = CallDataset(val_encodings, val_labels)

# ================================================
# Step 5: Define the Custom BERT Classifier Model
# ================================================

class BertClassifier(nn.Module):
    def __init__(self, model_name, dropout_prob=0.3):
        super(BertClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout_prob)
        # Adding an extra tanh activation layer before final linear layer.
        # Hidden size is taken from the BERT model config.
        hidden_size = self.bert.config.hidden_size  
        self.tanh = nn.Tanh()
        self.classifier = nn.Linear(hidden_size, 1)  # Binary classification

    def forward(self, input_ids, attention_mask):
        # Get outputs from BERT. The pooled output is the [CLS] token representation.
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # shape: (batch_size, hidden_size)
        x = self.dropout(pooled_output)
        x = self.tanh(x)           # tanh activation layer
        logits = self.classifier(x)
        return logits

# Instantiate the model
model = BertClassifier(MODEL_NAME)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# ================================================
# Step 6: Set Hyperparameters and Prepare Training
# ================================================

# Hyperparameters
EPOCHS = 4
BATCH_SIZE = 16
LEARNING_RATE = 2e-5

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Use AdamW optimizer
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

# Total training steps
total_steps = len(train_loader) * EPOCHS

# Learning rate scheduler
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=int(0.1 * total_steps),
                                            num_training_steps=total_steps)

# Loss function: since our model outputs raw logits and labels are float (0 or 1),
# we use BCEWithLogitsLoss (which applies a sigmoid internally).
criterion = nn.BCEWithLogitsLoss()

# ================================================
# Step 7: Training Loop
# ================================================

def train_epoch(model, data_loader, optimizer, scheduler, criterion, device):
    model.train()
    total_loss = 0
    for batch in data_loader:
        optimizer.zero_grad()
        # Move batch data to device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device).unsqueeze(1)  # shape: (batch_size, 1)
        
        # Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(outputs, labels)
        total_loss += loss.item()
        
        # Backward pass and optimization step
        loss.backward()
        optimizer.step()
        scheduler.step()
    avg_loss = total_loss / len(data_loader)
    return avg_loss

def eval_model(model, data_loader, criterion, device):
    model.eval()
    total_loss = 0
    predictions = []
    true_labels = []
    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device).unsqueeze(1)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            
            # Get predicted probabilities and then convert to binary predictions
            preds = torch.sigmoid(outputs)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    avg_loss = total_loss / len(data_loader)
    return avg_loss, predictions, true_labels

print("\nStarting training...\n")
for epoch in range(EPOCHS):
    train_loss = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)
    val_loss, val_preds, val_true = eval_model(model, val_loader, criterion, device)
    print(f"Epoch {epoch+1}/{EPOCHS}")
    print(f"  Training Loss: {train_loss:.4f}")
    print(f"  Validation Loss: {val_loss:.4f}")

# ================================================
# Step 8: Evaluate the Model
# ================================================

# Convert probabilities to binary labels (threshold = 0.5)
val_pred_labels = [1 if p >= 0.5 else 0 for batch in val_preds for p in batch.flatten()]
val_true_labels = [int(t) for batch in val_true for t in batch.flatten()]

print("\nClassification Report on Validation Set:")
print(classification_report(val_true_labels, val_pred_labels, target_names=['unsolved', 'solved']))

# ================================================
# Step 9: Apply the Model to "No Answer" Cases
# ================================================

# Filter "no_answer" cases and clean text
no_answer_df = df[df['label'] == 'no_answer'].copy()
no_answer_df['transcription'] = no_answer_df['transcription'].apply(clean_text)

# Tokenize these texts
no_answer_encodings = tokenize_texts(pd.Series(no_answer_df['transcription']))
# Create dataset and loader
no_answer_dataset = CallDataset(no_answer_encodings)
no_answer_loader = DataLoader(no_answer_dataset, batch_size=BATCH_SIZE, shuffle=False)

model.eval()
predictions = []
with torch.no_grad():
    for batch in no_answer_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        probs = torch.sigmoid(outputs)
        predictions.extend(probs.cpu().numpy())
no_answer_pred_labels = [1 if p >= 0.5 else 0 for batch in predictions for p in batch.flatten()]

# Map numerical predictions back to string labels
reverse_label_mapping = {0: 'unsolved', 1: 'solved'}
no_answer_df['predicted_label'] = no_answer_pred_labels
no_answer_df['predicted_label_str'] = no_answer_df['predicted_label'].map(reverse_label_mapping)

print("\nPredictions on 'No Answer' Cases:")
print(no_answer_df[['transcription', 'predicted_label_str']].head())

# ================================================
# Step 10: Save the Model and Tokenizer
# ================================================
MODEL_DIR = 'bertimbau_binary_classification_model'
if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)

# Save model state_dict and tokenizer
torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'pytorch_model.bin'))
tokenizer.save_pretrained(MODEL_DIR)
print(f"\nModel and tokenizer saved to '{MODEL_DIR}' directory.")

# ================================================
# Step: Inference on "No Answer" Cases (Both Probabilities and Predicted Labels)
# ================================================

# Filter "no_answer" cases and clean text
no_answer_df = df[df['label'] == 'no_answer'].copy()
no_answer_df['transcription'] = no_answer_df['transcription'].apply(clean_text)

# Tokenize these texts
no_answer_encodings = tokenize_texts(pd.Series(no_answer_df['transcription']))
# Create dataset and loader for inference
no_answer_dataset = CallDataset(no_answer_encodings)
no_answer_loader = DataLoader(no_answer_dataset, batch_size=BATCH_SIZE, shuffle=False)

model.eval()

all_probabilities = []  # To store probability values
all_pred_labels = []    # To store binary predictions

with torch.no_grad():
    for batch in no_answer_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        # Apply sigmoid to obtain probabilities
        probs = torch.sigmoid(outputs)
        # Convert probabilities to CPU numpy arrays
        probs = probs.cpu().numpy().flatten()
        all_probabilities.extend(probs.tolist())
        # Determine predicted label (threshold = 0.5)
        preds = (probs >= 0.5).astype(int)
        all_pred_labels.extend(preds.tolist())

# Add probabilities and predictions to the dataframe
no_answer_df['predicted_probability'] = all_probabilities
no_answer_df['predicted_label'] = all_pred_labels

# Map numerical predictions back to string labels (if desired)
reverse_label_mapping = {0: 'unsolved', 1: 'solved'}
no_answer_df['predicted_label_str'] = no_answer_df['predicted_label'].map(reverse_label_mapping)

print("\nPredictions on 'No Answer' Cases (with Probabilities):")
print(no_answer_df[['transcription', 'predicted_probability', 'predicted_label_str']].head())
