# bert_text_classification.py

# ================================================
# Imports
# ================================================

import pandas as pd
import re
import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import nlpaug.augmenter.word as naw
import os

# ================================================
# Step 1: Load the Dataset
# ================================================

# Path to your CSV file
DATA_PATH = 'customer_calls.csv'

# Load the dataset
df = pd.read_csv(DATA_PATH)

# Display the original label distribution
print("Original Label Distribution:")
print(df['label'].value_counts())

# ================================================
# Step 2: Data Cleaning
# ================================================

def clean_text(text):
    """
    Clean the input text by:
    - Lowercasing
    - Removing URLs
    - Removing email addresses
    - Removing unwanted characters (retain .,!?)
    - Removing extra spaces
    """
    text = str(text).lower()
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'\S+@\S+', '', text)  # Remove email addresses
    text = re.sub(r'[^a-zà-ÿ0-9\s.,!?]', '', text)  # Remove unwanted characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

# Apply cleaning to transcriptions
df['transcription'] = df['transcription'].apply(clean_text)

# Verify cleaning
print("\nSample Cleaned Transcriptions:")
print(df['transcription'].head())

# ================================================
# Step 3: Prepare Training Data
# ================================================

# Filter out "no_answer" cases for training
train_df = df[df['label'] != 'no_answer'].copy()

# Define label mapping for binary classification
# 1: solved, 0: unsolved
label_mapping = {'solved': 1, 'unsolved': 0}
train_df['label_binary'] = train_df['label'].map(label_mapping)

# Display new label distribution
print("\nBinary Label Distribution:")
print(train_df['label_binary'].value_counts())

# ================================================
# Step 4: Handle Class Imbalance
# ================================================

# Extract labels
labels = train_df['label_binary'].values

# Compute class weights
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(labels),
                                     y=labels)
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}
print("\nComputed Class Weights:", class_weights_dict)

# ================================================
# Step 5: Data Augmentation
# ================================================

# Initialize synonym augmenter
syn_aug = naw.SynonymAug(aug_p=0.3)  # 30% of words will be augmented

# Extract 'unsolved' samples
unsolved_df = train_df[train_df['label_binary'] == 0]

# Check if there are unsolved samples
if not unsolved_df.empty:
    # Apply augmentation
    augmented_texts = unsolved_df['transcription'].apply(lambda x: syn_aug.augment(x))
    
    # Create augmented DataFrame
    augmented_unsolved_df = pd.DataFrame({
        'transcription': augmented_texts,
        'label_binary': unsolved_df['label_binary']
    })
    
    # Append augmented data to training DataFrame
    train_df_augmented = pd.concat([train_df, augmented_unsolved_df], ignore_index=True)
    
    # Verify new label distribution
    print("\nLabel Distribution after Augmentation:")
    print(train_df_augmented['label_binary'].value_counts())
else:
    print("\nNo 'unsolved' samples to augment.")
    train_df_augmented = train_df.copy()

# ================================================
# Step 6: Tokenization with BERTimbau
# ================================================

# Load BERTimbau tokenizer
tokenizer = BertTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")

def tokenize_texts(texts, max_length=128):
    """
    Tokenize texts using BERT tokenizer
    """
    return tokenizer(
        texts.tolist(),
        padding='max_length',
        truncation=True,
        max_length=max_length,
        return_tensors='tf'
    )

# Tokenize training and validation texts
train_encodings = tokenize_texts(train_df_augmented['transcription'])
val_encodings = tokenize_texts(train_df_augmented['transcription'], max_length=128)

# ================================================
# Step 7: Split Data into Training and Validation Sets
# ================================================

# Since the dataset is small, we can use stratified splitting
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_df_augmented['transcription'].tolist(),
    train_df_augmented['label_binary'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=train_df_augmented['label_binary']
)

# Re-tokenize based on split
train_encodings = tokenize_texts(train_texts)
val_encodings = tokenize_texts(val_texts)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': train_encodings['input_ids'],
        'attention_mask': train_encodings['attention_mask']
    },
    train_labels
))

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': val_encodings['input_ids'],
        'attention_mask': val_encodings['attention_mask']
    },
    val_labels
))

# Batch size
BATCH_SIZE = 16

# Prepare datasets
train_dataset = train_dataset.shuffle(buffer_size=len(train_texts)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# ================================================
# Step 8: Build the BERT-Based Binary Classification Model
# ================================================

# Load pre-trained BERTimbau model
bert_model = TFBertModel.from_pretrained("neuralmind/bert-base-portuguese-cased")

# Define model inputs
input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')

# Get BERT outputs
bert_outputs = bert_model(input_ids, attention_mask=attention_mask)
pooled_output = bert_outputs.pooler_output  # [CLS] token

# Add dropout for regularization
dropout = Dropout(0.3)(pooled_output)

# Add output layer for binary classification
output = Dense(1, activation='sigmoid')(dropout)

# Create the model
model = Model(inputs=[input_ids, attention_mask], outputs=output)

# Display the model architecture
print("\nModel Summary:")
model.summary()

# ================================================
# Step 9: Compile the Model
# ================================================

# Compile the model with Binary Crossentropy loss and Adam optimizer
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)

# ================================================
# Step 10: Define Callbacks
# ================================================

# Early Stopping to prevent overfitting
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

# Model Checkpoint to save the best model
checkpoint_filepath = 'best_model.h5'
checkpoint = ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_loss',
    save_best_only=True,
    verbose=1
)

# ================================================
# Step 11: Train the Model
# ================================================

# Train the model
history = model.fit(
    train_dataset,
    epochs=10,  # Adjust as needed
    validation_data=val_dataset,
    callbacks=[early_stopping, checkpoint],
    class_weight=class_weights_dict
)

# ================================================
# Step 12: Evaluate the Model
# ================================================

# Load the best model weights
model.load_weights(checkpoint_filepath)

# Make predictions on validation set
val_predictions = model.predict(val_dataset)
val_pred_labels = (val_predictions.flatten() >= 0.5).astype(int)

# Extract true labels
val_true_labels = []
for batch in val_dataset:
    labels = batch[1].numpy()
    val_true_labels.extend(labels)

# Generate classification report
print("\nClassification Report:")
print(classification_report(val_true_labels, val_pred_labels, target_names=['unsolved', 'solved']))

# ================================================
# Step 13: Apply the Model to "No Answer" Cases
# ================================================

# Filter "no_answer" cases
no_answer_df = df[df['label'] == 'no_answer'].copy()

# Ensure transcriptions are cleaned
no_answer_df['transcription'] = no_answer_df['transcription'].apply(clean_text)

# Tokenize the "no answer" transcriptions
no_answer_encodings = tokenize_texts(no_answer_df['transcription'])

# Create TensorFlow dataset for prediction
no_answer_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids': no_answer_encodings['input_ids'],
        'attention_mask': no_answer_encodings['attention_mask']
    }
)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Make predictions on "no answer" dataset
no_answer_predictions = model.predict(no_answer_dataset)
no_answer_pred_labels = (no_answer_predictions.flatten() >= 0.5).astype(int)

# Add predictions to the "no answer" DataFrame
no_answer_df['predicted_label'] = no_answer_pred_labels

# Map numerical labels back to string labels
reverse_label_mapping = {0: 'unsolved', 1: 'solved'}
no_answer_df['predicted_label_str'] = no_answer_df['predicted_label'].map(reverse_label_mapping)

# Display the predictions
print("\nPredictions on 'No Answer' Cases:")
print(no_answer_df[['transcription', 'predicted_label_str']].head())

# ================================================
# Step 14: Save the Model and Tokenizer
# ================================================

# Directory to save the model and tokenizer
MODEL_DIR = 'bertimbau_binary_classification_model'

# Create the directory if it doesn't exist
if not os.path.exists(MODEL_DIR):
    os.makedirs(MODEL_DIR)

# Save the trained model
model.save(MODEL_DIR)

# Save the tokenizer
tokenizer.save_pretrained(MODEL_DIR)

print(f"\nModel and tokenizer saved to '{MODEL_DIR}' directory.")

# ================================================
# Step 15: Load the Model and Tokenizer (Optional)
# ================================================

# To load the model and tokenizer later, use the following code:

# from transformers import BertTokenizer
# import tensorflow as tf
# from tensorflow.keras.models import load_model

# # Load the tokenizer
# tokenizer_loaded = BertTokenizer.from_pretrained(MODEL_DIR)

# # Load the model (ensure to specify custom_objects for BERT layers)
# model_loaded = load_model(MODEL_DIR, custom_objects={'TFBertModel': TFBertModel})

# ================================================
# End of Script
# ================================================
