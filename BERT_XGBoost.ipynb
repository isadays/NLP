import pandas as pd
from transformers import TFBertModel, BertTokenizer
import tensorflow as tf
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load BERT model and tokenizer
model_name = "neuralmind/bert-base-portuguese-cased"
tokenizer = BertTokenizer.from_pretrained(model_name)
bert_model = TFBertModel.from_pretrained(model_name)

# Sample dataset
data = {
    "text": ["Eu amo este filme.", "O filme foi terrível.", "A comida estava ótima.", "Não gostei do serviço."],
    "label": [1, 0, 1, 0]  # 1: Positive, 0: Negative
}
df = pd.DataFrame(data)

# Tokenize the text
def tokenize_text(texts):
    return tokenizer(
        texts.tolist(),
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="tf"
    )

tokenized_inputs = tokenize_text(df["text"])

# Extract embeddings using BERT
embeddings = bert_model(tokenized_inputs["input_ids"])[1]

# Convert embeddings to numpy
X = embeddings.numpy()
y = df["label"].values

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost model
xgb_model = xgb.XGBClassifier()
xgb_model.fit(X_train, y_train)

# Predict on test data
y_pred = xgb_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

